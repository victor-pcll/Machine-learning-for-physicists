{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220f67e1-fc90-48ec-8b3d-47d55bf6854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd98db-e756-46bf-97b4-8369ff5a0a44",
   "metadata": {},
   "source": [
    "# Clustering with K-means\n",
    "\n",
    "In this exercise we revisit the k-means algorithm. First you will implement the algorithm and run it on a Gaussian mixture toy-dataset, then apply it to a dataset of pulsar candidate observations.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5498b949-d631-4703-8586-0a4096ec16d5",
   "metadata": {},
   "source": [
    "### 1) Gaussian mixture data\n",
    "Write a function which generates toy-data drawn from a Gaussian mixture model. The samples are drawn from $k$ Gaussian clusters of random means and uniform covariance $\\Sigma = \\text{std\\_dev} \\times \\mathbb{I}_{dxd}$. Each cluster has `samples_per_component` elements, and its mean is sampled randomly from the uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62769873-05bf-4a10-8ac7-5db2cec5af32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_gaussian_mixture_data(k : int, samples_per_component : int, d : int =2, mean_range : tuple = (-10, 10), std_dev : float =1.0):\n",
    "    \"\"\"\n",
    "    Generate data from a Gaussian mixture model.\n",
    "\n",
    "    Parameters:\n",
    "        - k : Number of Gaussian components\n",
    "        - samples_per_component : Number of samples from each of the k clusters to generate.\n",
    "        - mean_range : If mean_range = (a, b) each of the d coordinates for the mean of each component will be sampled uniformly from the interval [a, b].\n",
    "        - std_dev : standard deviation for each component.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Generated dataset with shape (k*samples_per_component, d).\n",
    "    \"\"\"\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0acdd0-9297-49cb-ae46-7cba261dc727",
   "metadata": {},
   "source": [
    "Draw a $d = 2$ dataset with some $k\\sim \\mathcal{O}(1)$ and plot the data points in the 2-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb85ebb-61e7-4c7a-b04c-38f112fba09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0d98b-a295-4c64-a861-3daf18a9c399",
   "metadata": {},
   "source": [
    "### 2) K-means implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7d7cb-0c28-47b8-8cd0-9fa544196ac2",
   "metadata": {},
   "source": [
    "Implement the k-means algorithm. Return the estimated cluster means and cluster asssignments of each iteration, not only the final state.\n",
    "\n",
    "Your algorithm should stop after `max_iters` iterations or when the change in the means of the clusters $$ \\sum_{i = 1}^k \\| \\mu_i^{t+1} - \\mu_i^{t} \\|^2$$ is less than `tol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384b49ed-2646-444f-af9a-a066abe1bd5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kmeans(data : np.ndarray, k, max_iters : int =100, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Implements the K-Means clustering algorithm and tracks the history of centroids and assignments.\n",
    "\n",
    "    Parameters:\n",
    "        - data : n x d matrix containing the data points\n",
    "        - k    : Number of clusters.\n",
    "        - max_iters (int): Maximum number of iterations.\n",
    "        - tol (float): Iteration stops if all centroids have moved less than tol\n",
    "\n",
    "    Returns:\n",
    "        - final_centroids, final_cluster_assignments, centroids_history, assignments_history)\n",
    "    \"\"\" \n",
    "    # 1) Create empty lists to append centroids and assignments (cluster index of each data point) from each iteration\n",
    "    centroids_history   = []\n",
    "    # assignments_history[i][j] = k if at iteration i, cluster of j is k\n",
    "    assignments_history = []\n",
    "    \n",
    "    # 2) Randomly initialize the centroids \n",
    " \n",
    "    for _ in range(max_iters):\n",
    "        pass\n",
    "        # 3) Assign data points to the nearest centroid\n",
    " \n",
    "        # 4) Update the centroids\n",
    "        \n",
    "        # 5) Check for convergence and if the the centroids move less than tol, break\n",
    "        \n",
    "    # Assign data points to final centroids\n",
    "    \n",
    "    return centroids, cluster_assignments, centroids_history, assignments_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e41c5-73f9-48d5-bcd4-25e6c046789d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Apply your implementation to the Gaussian mixture toy-data, and use the provided ```plot_kmeans_iterations()``` function to visualize how centroids and cluster assignments evolve over the iterations.  \n",
    "If you like, play around with the number of clusters in the data and in the algorithm, or vary how well-seperated the data clusters are, and observe the k-means dynamics. Do you notice failure modes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27944e83-d50a-41ca-8dfe-bc9cbf734d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_kmeans_iterations(data, centroids_history, assignments_history):\n",
    "    \"\"\"\n",
    "    Plots K-Means iterations showing centroids and data point assignments.\n",
    "\n",
    "    Parameters:\n",
    "    data (np.array of shape (n_samples,2)): Data points.\n",
    "    centroids_history (list of (k,2) shape np.arrays): List of the centroids at each iteration.\n",
    "    assignments_history (list of (k) shape np.arrays): List of data point assignments at each iteration.\n",
    "    \"\"\"\n",
    "    n_iters = len(centroids_history)\n",
    "    plt.figure(figsize=(15, 5 * n_iters))\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        plt.subplot(n_iters, 1, i + 1)\n",
    "        plt.scatter(data[:, 0], data[:, 1], c=assignments_history[i], alpha=0.5, marker='.')\n",
    "        plt.scatter(centroids_history[i][:, 0], centroids_history[i][:, 1], c='red', marker='X')\n",
    "        plt.title(f'Iteration {i+1}')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb1e10a-b32d-40c3-8eea-bee3c5fd295c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply kmeans() to the toy-data and plot the iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303e136-9a6e-4026-8ddc-8d50673cfb4f",
   "metadata": {},
   "source": [
    "### 3) Clustering of pulsar observation data\n",
    "We now consider a dataset of pulsar candidates from the HTRU survey. You can download the data and read the description of the data on the [UCI machine learning repository](https://archive.ics.uci.edu/dataset/372/htru2). The .csv file is also provided on Moodle with the exercise. Our goal is to see in how far pulsars and false pulsar candidates are represented by two different clusters which we can automatically discover via clustering. While this is a very simple method, k-means has been applied extensively in scientific data analysis, for an example from astrophysics similar to our pulsar candidate clustering here, see [Turner et al. 2019](https://academic.oup.com/mnras/article/482/1/126/5116177) who are clustering galaxy survey data. But now to the pulsar candidates.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e21d9-1e92-464a-b238-3d9de30dd367",
   "metadata": {},
   "source": [
    "Read the description and familiarize yourself with the data. Load it from the ```.csv``` file, for example using ```np.loadtxt()```, then investigate some basic questions such as:*\n",
    "- *How many samples and features are there?*\n",
    "- *What is the mean and variance of the different features?*\n",
    "- *How many true pulsars and how many non-pulsars are in the data?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e87c27c-7aba-4c02-8e8d-be37528f2683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a259440-fc04-47db-bd84-fd0fb4d9800e",
   "metadata": {},
   "source": [
    "Remove the column containing the labels, since we want to do unsupervised learning. The ground-truth labels will be used later to assess our clustering result.*\n",
    "* Before using  k-means on the data, preprocess the data by standardizing it, e.g. ```sklearn.preprocessing.StandardScaler```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0046dde-0333-47bc-b47a-12069ff2953f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9d6f3-e9e2-4e4f-93eb-8ee6412ef26c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Look for the k-means implementation in sklearn and apply it to the data. Use n_clusters=2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5066304-2a0a-4af6-972f-0b6cf4e40bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f9312-7301-48d9-b5e9-137113ede6ba",
   "metadata": {},
   "source": [
    "Get the cluster assignment of each sample. We want to compare these to the true labels and evaluate the performance of our clustering. To do so, we'll use the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfebcef1-0e7c-4a5c-b26d-f364e75382a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the cluster assignments and the true labels, then print the confusion matrix\n",
    "\n",
    "# Also print the accuracy of the clustering (the labels may need to be flipped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f2429-fd3f-4423-9617-f1ed9a7cd61f",
   "metadata": {},
   "source": [
    "Use the plotting function provided below, and inspect the resulting 2d slices of the clusters colored by cluster index and true label. Read through the function to see how it handles the coloring and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e636b3fa-5ded-499f-b081-4b1a4513c690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pulsar_clusters_vs_labels(X, y, y_pred):\n",
    "    \"\"\"\n",
    "    Plots the results of k-means clustering against actual labels.\n",
    "\n",
    "    This function visualizes the comparison between k-means clustering predictions \n",
    "    (y_pred) and actual labels (y) for each pair of features in the dataset. \n",
    "    It uses different colors to indicate true positives, true negatives, false negatives,\n",
    "    and false positives.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The feature data set. Each row represents a sample, \n",
    "                       and each column represents a feature.\n",
    "    y (numpy.ndarray): The actual labels for each sample. This array is 1-dimensional.\n",
    "    y_pred (numpy.ndarray): The predicted labels from k-means clustering. \n",
    "                            This array is 1-dimensional.\n",
    "\n",
    "    The function creates scatter plots for each pair of features in the data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_names = ['Mean of the integrated profile',\n",
    "                     'Standard deviation of the integrated profile',\n",
    "                     'Excess kurtosis of the integrated profile',\n",
    "                     'Skewness of the integrated profile',\n",
    "                     'Mean of the DM-SNR curve',\n",
    "                     'Standard deviation of the DM-SNR curve',\n",
    "                     'Excess kurtosis of the DM-SNR curve',\n",
    "                     'Skewness of the DM-SNR curve']\n",
    "\n",
    "    # making a coloring array for scatterplots\n",
    "    lightred = np.array([0.9, 0, 0])\n",
    "    red = np.array([0.4, 0, 0])\n",
    "    yellow = np.array([0.9, 0.9, 0])\n",
    "    blue = np.array([0, 0, 0.9])\n",
    "\n",
    "    data_coloring = np.zeros((y.size, 3))\n",
    "    data_coloring[(y_pred == 1) & (y == 1)] = yellow  # True positive\n",
    "    data_coloring[(y_pred == 0) & (y == 0)] = blue    # True negative\n",
    "    data_coloring[(y_pred == 0) & (y == 1)] = red     # False negative\n",
    "    data_coloring[(y_pred == 1) & (y == 0)] = lightred # False positive\n",
    "\n",
    "    for i in range(4):\n",
    "        dims = [2 * i, 2 * i + 1]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.title('k-means prediction vs. label')\n",
    "        plt.scatter(X[:, dims[0]], X[:, dims[1]], c=data_coloring, marker='.', alpha=0.6)\n",
    "        plt.xlabel(feature_names[dims[0]])\n",
    "        plt.ylabel(feature_names[dims[1]])\n",
    "\n",
    "        # Adding legend\n",
    "        plt.scatter([], [], color=yellow, label='True Positive')\n",
    "        plt.scatter([], [], color=blue, label='True Negative')\n",
    "        plt.scatter([], [], color=red, label='False Negative')\n",
    "        plt.scatter([], [], color=lightred, label='False Positive')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0a14fa9-cda4-47c3-8130-5cee5275ac82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call plot_pulsar_clusters_vs_labels()\n",
    "# In case the kmeans 0 labels correspond best to the ground truth 1 label, flip them if you have not done already"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
